{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ee8928",
   "metadata": {},
   "source": [
    "# Day 1 - NB1 - Googling and finding relevant root resources\n",
    "\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Step 1 - Gather possibly relevant resources](#Step-1---Gather-possibly-relevant-resources)\n",
    "- [Introduction video 1: Intro to DL for Audio and Speech Applications](#Introduction-video-1:-Intro-to-DL-for-Audio-and-Speech-Applications)\n",
    "- [Introduction video 2 (Stanford Seminar)](#Introduction-video-2-(Stanford-Seminar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1245",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "It is possible that in few months I will start working in a healthtech project involving speech analysis. Since this field interests me by itself and I'm not familiar with it, I have decided to start studying it by myself.\n",
    "\n",
    "This repo is the result and the documentation of my studies as they go forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebee94e",
   "metadata": {},
   "source": [
    "# Step 1 - Gather possibly relevant resources\n",
    "\n",
    "These resources should allow me to start digging in the field, being the starting point of a tree of links and useful resources.\n",
    "\n",
    "## Theoretical introductions: \n",
    "* <a href=\"https://www.youtube.com/watch?v=dBAn67ZKbZ4\" >Introduction to Deep Learning for Audio and Speech Applications - YouTube</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=RBgfLvAOrss\">Stanford Seminar - Deep Learning in Speech Recognition - YouTube</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\">Speech Recognition - Wikipedia</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Speech_processing\">Speech Processing - Wikipedia</a>\n",
    "\n",
    "\n",
    "## Kaggle speech competitions\n",
    "\n",
    "* <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/code?competitionId=7634&amp;sortBy=voteCount\">TensorFlow Speech Recognition Challenge | Kaggle</a>\n",
    "* <a href=\"https://www.kaggle.com/c/rfcx-species-audio-detection/overview\" >Rainforest Connection Species Audio Detection | Kaggle</a>\n",
    "* <a href=\"https://www.kaggle.com/c/birdclef-2021/code?competitionId=25954&amp;sortBy=voteCount\" >BirdCLEF 2021 - Birdcall Identification | Kaggle</a>\n",
    "\n",
    "\n",
    "## Other Kaggle resources\n",
    "* <a href=\"https://www.kaggle.com/davids1992/speech-representation-and-data-exploration\" >Speech representation and data exploration | Kaggle</a>\n",
    "* <a href=\"https://www.kaggle.com/alexozerin/end-to-end-baseline-tf-estimator-lb-0-72\">End-to-end baseline TF Estimator LB 0.72 | Kaggle</a>\n",
    "* <a href=\"https://www.kaggle.com/nandhuelan/wav2vec-wandb-learning-audio-representation/data\">Wav2vec+wandb- Learning audio representation ðŸ”¥ðŸ¤— | Kaggle</a>\n",
    "\n",
    "\n",
    "## Datasets, models and papers\n",
    "* <a href=\"https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html\" >Google AI Blog: Launching the Speech Commands Dataset</a>\n",
    "* <a href=\"https://huggingface.co/anton-l/wav2vec2-random-tiny-classifier\" >anton-l/wav2vec2-random-tiny-classifier Â· Hugging Face</a>\n",
    "* <a href=\"https://huggingface.co/docs/transformers/model_doc/wav2vec2\">Wav2Vec2</a>\n",
    "* <a href=\"https://arxiv.org/abs/2006.11477\" >[2006.11477] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a>\n",
    "\n",
    "\n",
    "## More resources\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=Qf4YJcHXtcY\">13. Speech Recognition with Convolutional Neural Networks in Keras/TensorFlow</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=9GJ6XeB-vMg\">Speech Recognition in Python</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=qV4lR9EWGlY\">Sound: Crash Course Physics #18</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=PWVH3Vx3dCI\">Speech Recognition Using Python | How Speech Recognition Works In Python | Simplilearn</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9fc25",
   "metadata": {},
   "source": [
    "# Introduction video 1: Intro to DL for Audio and Speech Applications\n",
    "**<a href=\"https://www.youtube.com/watch?v=dBAn67ZKbZ4\" >Introduction to Deep Learning for Audio and Speech Applications - YouTube (2019)</a>**\n",
    "\n",
    "## Sections 1 and 2\n",
    "\n",
    "* <a href=\"https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html\" >Google AI Blog: Launching the Speech Commands Dataset</a>\n",
    "* Preprocessing is common in audio DNNs. Time-frequency transform or Spectrogram. In these cases, CNNs are used.\n",
    "\n",
    "![](imgs/cnn.png)\n",
    "\n",
    "* Other popular nets are LSTMS. In this case, feature extraction is different. Time-varying features are mined.\n",
    "* Spectrogram. MEL Spectrogram.\n",
    "* Paper: CNNs for small-footprint Keyword Spotting (2015).\n",
    "* Automate labelling: Use another's model speech2text\n",
    "* Tools for manual labelling\n",
    "* Data augmentation. For example, for keyword spotting:\n",
    "  * Accelerate, change pitch, change volume, environments (echo), backgrounds\n",
    "\n",
    "## Common algorithms for feature extraction, time-frequency transforms, and segmentation:\n",
    "\n",
    "Feature extraction: (produce lower-rate time-varying metrics)\n",
    "* MFCC - Mel Frequency Cepstral Coefficients\n",
    "* GTCC - Gammatone Cepstral Coefficients\n",
    "* Pitch and Harmonicity\n",
    "* 11 Spectral Descriptors\n",
    "* Wavelet scattering\n",
    "\n",
    "Transformations:\n",
    "* Mel-Spaced Spectrfogram\n",
    "* Gammatone and Octave Filter Banks\n",
    "* Modified Discrete Cosine Transform\n",
    "\n",
    "Segmentation:\n",
    "* **Voice Activity and Speech Detection**\n",
    "\n",
    "### Mel Spectrogram (and MFCC)\n",
    "\n",
    "* NFFT\n",
    "  * FFT = Fast Fourier Transform \n",
    "  * N apparently stands for Non-negative as in NMF\n",
    "  * See: [Nonequispaced fast Fourier transform](https://www-user.tu-chemnitz.de/~potts/nfft/)\n",
    "  * FTs transform a wave into the \"frequencies domain\" (must dig into FFT)\n",
    "  \n",
    "![](imgs/spectrogram-and-mel-filterbank.png)\n",
    "\n",
    "* This is applying FFT to the signal\n",
    "* Frequency resolution is a parameter\n",
    "* Enough resolution at lower frequencies (important for humans) means having too much information for higher frequencies. Redundant data.\n",
    "* MEL Spectrogram - We project the raw FFT points into a series of predefined buckets (bands) that are representative of human perception. MEL Filterbank. Cuasi logarthimical  y-axis. Only 40 bands. Sustainbly-sized spectrogram\n",
    "\n",
    "### Mel Frequency and Gammatone Cepstral Coefficients (MFCC / GTCC)\n",
    "\n",
    "* MFCC = Uses Mel Filterbank to put info into predefined bands\n",
    "* GTCC = Uses Gammatone Filterbank for the same purpose. The filterbank is more complex and accuracte.\n",
    "\n",
    "* Both are some calculations on top of the filtered Spectrogram and a Log Energy calculation. From the picture, the output seems to be 2 numbers or two arrays.\n",
    "\n",
    "* DCT \"are like real FFTs\".\n",
    "\n",
    "![](imgs/mfcc.png)\n",
    "\n",
    "* [Cepstrum  Wikipedia](https://en.wikipedia.org/wiki/Cepstrum)\n",
    "\n",
    "\n",
    "#### Example tasks:\n",
    "Classification\n",
    "* Speech command recognition\n",
    "* Acoustic scene recognition\n",
    "* Classify gender\n",
    "\n",
    "Regression or seq-to-seq:\n",
    "* Cocktail party source separation\n",
    "* Denoising\n",
    "* Voice activity detection\n",
    "\n",
    "Features: Mel Spectrogram, [STFT](https://en.wikipedia.org/wiki/Short-time_Fourier_transform), Time-varying features (Spectral descriptors, harmonic ratio)\n",
    "\n",
    "Architectures: CNNs, FCs, and LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36181944",
   "metadata": {},
   "source": [
    "# Introduction video 2 (Stanford Seminar)\n",
    "**[Stanford Seminar - Deep Learning in Speech Recognition (30 nov 2017)](https://youtu.be/RBgfLvAOrss?t=1799)**\n",
    "\n",
    "* Speech Recognition starts in minute 30\n",
    "* ASR = Automatic Speech Recognition\n",
    "\n",
    "## History\n",
    "\n",
    "![](imgs/lm.png)\n",
    "\n",
    "A language model acts as a bayesian prior!!\n",
    "\n",
    "Does it say \"Colloquium\" or \"Collaquium\"? It's not word-level though, W is a sequence, not a word.\n",
    "\n",
    "The lambda acts as a weight of the acoustic and the language model\n",
    "\n",
    "For the acoustic model, HMM. The DRAGON system ('70).\n",
    "Neural Networks in the '90. Hinton. Phoneme Recognition Using Time-Delay NN.\n",
    "\n",
    "Historical datasets:\n",
    "* Resource Management ('80) - Naval\n",
    "* ATIS\n",
    "* WSJ\n",
    "* Conversational Speech\n",
    "\n",
    "2006 - Hinton - End of Winter\n",
    "* Reducing the Dimensionality of Data with NNs\n",
    "* Deep Boltzman Machines\n",
    "\n",
    "Autoencoders\n",
    "\n",
    "### DL in Speech Recognition\n",
    "\n",
    "* HMMs still here\n",
    "* Criterion: Cross-entropy $\\rightarrow$ MMI Sequence-level\n",
    "  * _Did I get the sentence right?_\n",
    "* Features: MFCC $\\rightarrow$ Filter banks\n",
    "* Batch norm, Distributed SGD, Dropout\n",
    "* Acousting Modeling: CNN / CTC (Connection temporal classification) / CLDNN (LSTMs)\n",
    "* LMs: RNNs (As the ex president would say: \"Things happened\")\n",
    "\n",
    "* Task: Speech synthesis (the voice of Siri)\n",
    "   * Prosody generation. Prosodic. Spectral. \n",
    "   * TTS = Text-to-speech\n",
    "   * Viterbi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
